# -*- coding: utf-8 -*-
"""ProjectAkhir_ML_Mahir_Muhammad Fadhil Amri.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kUl2b_xIZaSP9CqFubpomzy5M6k35fUT

# Identitas Developer
> Nama : Muhammad Fadhil Amri <br>
> Email : fadhil.amri131202@gmail.com <br>
> Domisili: Kota Bandung <br>
> Pekerjaan : Mahasiswa Teknik Informatika ITB <br>

# Intro
> Proyek ini adalah proyek Machine Learning dalam membuat recommender system menggunakan content-based filtering dan colaborative filtering <br><br>

# Program

## 1. Import Data

Sumber Data: https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination
"""

import gdown

URL_DATA_RATING = "https://drive.google.com/uc?id=1rgwNp7OP4GjI3AJUYrWbGdVigcIarjjL"
gdown.download(URL_DATA_RATING, "rating.csv")
URL_DATA_DESTINASI = "https://drive.google.com/uc?id=1rfFApK8NSaeks7kOjxkk5hp04gdvQuvi"
gdown.download(URL_DATA_DESTINASI, "destinasi.csv")
URL_DATA_USER = "https://drive.google.com/uc?id=1raZbYS4Q9VNxh-yOfWh4VwKv6CPKPtgq"
gdown.download(URL_DATA_USER, "user.csv")

"""## 2. Load Data"""

import pandas as pd

rating = pd.read_csv("/content/rating.csv")
destinasi = pd.read_csv("/content/destinasi.csv")
user = pd.read_csv("/content/user.csv")

print("Jumlah Data Rating Destinasi Wisata: ", rating.shape[0])
print("Jumlah Data Tempat Destinasi Wisata: ", destinasi.shape[0])
print("Jumlah Data User: ", user.shape[0])

rating.head()

destinasi.head()

user.head()

"""## 3. Exploratory Data Analysis

### Statistika Deskriptif
"""

destinasi.info()

destinasi.describe()

rating.info()

rating.describe()

user.info()

user.describe()

"""### Missing value Detection"""

destinasi.isna().sum()

# Cek data yang memiliki nilai kosong pada fitur Time_Minutes
destinasi[destinasi["Time_Minutes"].isna()]["Place_Name"].values

"""Dari tabel tersebut dapat dilihat bahwa nilai Time_Minutes banyak memiliki nilai yang hilang pada destinasi-destinasi yang bersifat terbuka. Pada destinasi-destinasi tersebut, durasi kunjungan memiliki keragaman yang tinggi."""

rating.isna().sum()

user.isna().sum()

"""### Duplicate Data Detection"""

destinasi[destinasi.duplicated(subset=["Place_Name"])]

len(destinasi["Place_Name"].unique())

"""Dapat dilihat bahwa tidak ada data yang duplicate pada data destinasi

### Univariate Analysis
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot distribusi nilai

for col in ["City", "Category",  "Price", "Rating"]:
  plt.title(f'Distribusi  {col}')
  sns.histplot(data=destinasi[col], label=col, fill=True)
  plt.xticks(rotation=90)
  plt.show()

destinasi["City"].value_counts()

plt.title(f'Distribusi  Rating')
sns.histplot(data=rating["Place_Ratings"], label=col, fill=True)
plt.show()

for col in ["Location", "Age"]:
  plt.title(f'Distribusi  {col}')
  sns.histplot(data=user[col], label=col, fill=True)
  plt.xticks(rotation=90)
  plt.show()

user["Location"].value_counts()

"""### Outliers Detection"""

sns.boxplot(x="Price", data=destinasi)
plt.show()

"""## 4. Data Preparation

### Missing Value Handling
"""

# Fill missing value pada fitur time_minutes dengan mean
time_minutes_mean = destinasi["Time_Minutes"].mean()
destinasi["Time_Minutes"].fillna(time_minutes_mean, inplace=True)
destinasi.isna().sum()

"""### Feature Selection

Fitur yang akan didrop adalah Description, Rating, Coordinate, Lat, Long, Unnamed: 11, Unnamed: 12.
- Description didrop karena hanya memberikan keterangan destinasi sehingga tidak berpengaruh dalam model sistem rekomendasi. Informasi penting dari description telah terdapat pada category dan city.
- Rating didrop karena data rating yang lebih detail ada pada dataset rating sehingga data yang digunakan nantinya adalah data pada dataset rating yang mampu menggambarkan preferensi pengguna dengan lebih baik.
- Coordinate, lat, dan long didrop karena telah terdapat fitur city yang lebih informatif untuk memberikan keterangan lokasi.
- Unnamed: 11 dan Unnamed: 12 didrop karena tidak memberikan informasi apa-apa.
"""

destinasi.drop(columns=["Description", "Rating", "Coordinate", "Lat", "Long", "Unnamed: 11", "Unnamed: 12"], inplace=True)
destinasi.head()

"""### Feature Engineering
Format data yang digunakan adalah setiap instance hanya memiliki satu nilai Category.
"""

# Nilai category pada setiap instance telah unique, tetapi terdapat space yang bisa menyebabkan Value akan terpisah pada saat Tf-idf vectorization. Untuk
# itu perlu dilakukan pemrosesan dalam menggabungkan value category tanpa space

destinasi["Category"].unique()

def format_category(cat):
  cat_arr = cat.split(' ')
  return ('_').join(cat_arr)

destinasi["Category"] = destinasi["Category"].apply(format_category)
destinasi["Category"].unique()

# Scaling Price using MinMaxScaling
min_price = destinasi["Price"].min()
max_price = destinasi["Price"].max()

def min_max_scale_price(x):
  return (x-min_price)/(max_price-min_price)

# Scaling rating
destinasi["Price"] = destinasi["Price"].apply(min_max_scale_price)
destinasi.describe()

# Scaling rating using MinMaxScaling
min_rate = rating["Place_Ratings"].min()
max_rate = rating["Place_Ratings"].max()

def min_max_scale_rate(x):
  return (x-min_rate)/(max_rate-min_rate)

# Scaling rating
rating["Place_Ratings"] = rating["Place_Ratings"].apply(min_max_scale_rate)
rating.describe()

"""### TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf_idf = TfidfVectorizer()

## Category

# Fit dan transformasi ke dalam bentuk matriks
tf_idf_matrix_category = tf_idf.fit_transform(destinasi['Category'])

# Melihat ukuran matrix tfidf
tf_idf_matrix_category.shape

tf_idf_category_df = pd.DataFrame(
  tf_idf_matrix_category.todense(),
  columns=tf_idf.get_feature_names_out(),
  index=destinasi["Place_Name"]
)

tf_idf_category_df

## City

# Fit dan transformasi ke dalam bentuk matriks
tf_idf_matrix_city = tf_idf.fit_transform(destinasi['City'])

# Melihat ukuran matrix tfidf
tf_idf_matrix_city.shape

tf_idf_city_df = pd.DataFrame(
  tf_idf_matrix_city.todense(),
  columns=tf_idf.get_feature_names_out(),
  index=destinasi["Place_Name"]
)

tf_idf_city_df

import numpy as np
from scipy.sparse import hstack, csr_matrix

tf_idf_matrix = hstack([tf_idf_matrix_category, tf_idf_matrix_city, csr_matrix(destinasi["Price"].values.reshape(-1, 1))])
tf_idf_matrix.todense()

"""### Encoding"""

# Mengubah User_Id menjadi list tanpa nilai yang sama
user_ids = rating['User_Id'].unique().tolist()
print('list User_Id: ', user_ids)

# Melakukan encoding User_Id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_Id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User_Id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User_Id: ', user_encoded_to_user)

# Mengubah Place_Id menjadi list tanpa nilai yang sama
place_ids = rating['Place_Id'].unique().tolist()
print('list Place_Id: ', place_ids)

# Melakukan encoding Place_Id
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
print('encoded Place_Id : ', place_to_place_encoded)

# Melakukan proses encoding angka ke ke Place_Id
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}
print('encoded angka ke Place_Id: ', place_encoded_to_place)

rating["user"] = rating["User_Id"].map(user_to_user_encoded)
rating["place"] = rating["Place_Id"].map(place_to_place_encoded)

"""## 5. Modeling

### Content-Based Filtering
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tf_idf_matrix)
cosine_sim

cosine_df = pd.DataFrame(cosine_sim, index=destinasi["Place_Name"], columns=destinasi["Place_Name"])

cosine_df

def destination_recommendations(nama_tempat, similarity_data=cosine_df, items=destinasi[['Place_Name', 'Category', "City", "Price"]], k=5):
      """
      Rekomendasi Tempat Wisata berdasarkan kemiripan dataframe

      Parameter:
      ---
      nama_tempat : tipe data string (str)
                  Nama Tempat (index kemiripan dataframe)
      similarity_data : tipe data pd.DataFrame (object)
                        Kesamaan dataframe, simetrik, dengan resto sebagai
                        indeks dan kolom
      items : tipe data pd.DataFrame (object)
              Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
      k : tipe data integer (int)
          Banyaknya jumlah rekomendasi yang diberikan
      ---


      Pada index ini, kita mengambil k dengan nilai similarity terbesar
      pada index matrix yang diberikan (i).
      """


      # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
      # Dataframe diubah menjadi numpy
      # Range(start, stop, step)
      index = similarity_data.loc[:,nama_tempat].to_numpy().argpartition(
          range(-1, -k, -1))

      # Mengambil data dengan similarity terbesar dari index yang ada
      closest = similarity_data.columns[index[-1:-(k+2):-1]]

      # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
      closest = closest.drop(nama_tempat, errors='ignore')

      return pd.DataFrame(closest).merge(items).head(k)

target_destinasi = destinasi[destinasi["Place_Name"]=="Kota Tua"][["Place_Name", "Category", "City", "Price"]]
recommendation = destination_recommendations("Kota Tua")

print("Destinasi input")
target_destinasi

print("Top-5 Recommendation")
recommendation

"""### Collaborative Filtering"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Split Data
from sklearn.model_selection import train_test_split


X = rating[["user", "place"]].values
y = rating['Place_Ratings'].values


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Init variable
num_users = len(rating["User_Id"])
num_places = len(rating["Place_Id"])

print("num_user:", num_users)
print("num_places:", num_places)

class RecommenderNet(tf.keras.Model):
  #Inisialisasi
  def __init__(self, num_users, num_places, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_places = num_places
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = "he_normal",
        embeddings_regularizer = keras.regularizers.l2()
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.place_embedding = layers.Embedding(
        num_places,
        embedding_size,
        embeddings_initializer = "he_normal",
        embeddings_regularizer = keras.regularizers.l2()
    )
    self.place_bias = layers.Embedding(num_users, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])
    place_vector = self.place_embedding(inputs[:, 1])
    place_bias = self.place_bias(inputs[:, 1])

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias +place_bias

    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_places, 50) #Inisialisasi model

# Model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

# Training

history = model.fit(
    x=X_train,
    y = y_train,
    batch_size = 30,
    epochs = 30,
    validation_data = (X_val, y_val)
)

user_id = rating["User_Id"].sample(1).iloc[0]
place_visited_by_user = rating[rating["User_Id"]==user_id]

place_not_visited = destinasi[~destinasi["Place_Id"].isin(place_visited_by_user["Place_Id"].values)]["Place_Id"]

place_not_visited = list(
    set(place_not_visited).intersection(set(place_to_place_encoded.keys()))
)

place_not_visited = [[place_to_place_encoded.get(x) ] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)

user_place_array = np.hstack(
    ([[user_encoder]]*len(place_not_visited), place_not_visited)
)

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-5:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Destination with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

destinasi_rows = destinasi[destinasi['Place_Id'].isin(top_place_user)]
for row in destinasi_rows.itertuples():
    print(row.Place_Name, ':', row.Category, ",", row.City, ",", row.Price)

print('----' * 8)
print('Top 5 destination recommendation')
print('----' * 8)

recommended_place = destinasi[destinasi['Place_Id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.Place_Name, ':', row.Category, ",", row.City, ",", row.Price)

"""## 6. Evaluasi

### Content-Based Filtering
"""

n_test = round(0.2*len(destinasi))
place_testing = destinasi["Place_Name"].sample(n_test).values

relevant = 0

for place in place_testing:
  target_destinasi = destinasi[destinasi["Place_Name"]==place][["Place_Name", "Category", "City", "Price"]]
  recommendation = destination_recommendations(place)
  # Count the number of relevant recommendation
  for rec in recommendation.itertuples():
    if (rec.Category == target_destinasi["Category"].values):
      relevant += 1
    if (rec.City == target_destinasi["City"].values):
      relevant += 1
    relevant += (1-abs(rec.Price-target_destinasi["Price"].values))

print("Precision:", (relevant/(3*5*n_test))[0])

"""### Collaborative Filtering"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()